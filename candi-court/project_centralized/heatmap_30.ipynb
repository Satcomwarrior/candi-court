{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Utilization Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nsys_display\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput\n",
    "\n",
    "This line graph displays the summary of all operations on NFS, Lustre, Local and NVMe-oF file systems\n",
    "for the profiled volumes:\n",
    "* x axis represents the rank duration, scaling from 0 to the maximum duration across all ranks.\n",
    "* y axis represents the mean Bytes read and written across all ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the throughput DataFrame from the parquet file\n",
    "throughput_df = pd.read_parquet(\"throughput_analysis.parquet\")\n",
    "\n",
    "if throughput_df.empty:\n",
    "    display(Markdown(\"No throughput data to display.\"))\n",
    "else:\n",
    "    # Create unique name.\n",
    "    throughput_df[\"Name\"] = (\n",
    "        throughput_df[\"Rank\"].astype(str)\n",
    "        + \"/\"\n",
    "        + throughput_df[\"Hostname\"].astype(str)\n",
    "        + \"/\"\n",
    "        + throughput_df[\"Volume\"].astype(str)\n",
    "        + \"/\"\n",
    "        + throughput_df[\"Name\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # Convert ns to seconds\n",
    "    throughput_df[\"Duration\"] = throughput_df[\"Duration\"] * 1e-9\n",
    "\n",
    "    throughput_metrics = [\"Read\", \"Write\"]\n",
    "\n",
    "    nsys_display.display_summary_graph(\n",
    "        throughput_df,\n",
    "        throughput_metrics,\n",
    "        xaxis_title=\"Duration (s)\",\n",
    "        yaxis_title=\"Value\",\n",
    "        title=\"Usage Summary (bins=REPLACE_BIN)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These heatmaps display NFS, Lustre, Local and NVMe-oF filesystem operations which are collected using NVTX counters\n",
    "via the storage-metrics plugin and the --storage-metrics feature:\n",
    "* x axis represents the rank duration, scaling from 0 to the maximum duration across all ranks.\n",
    "* y axis represents the set of Rank/Hostname/Volume/Device name for which metrics were collected.\n",
    "\n",
    "The heatmaps present:\n",
    "* Bytes read\n",
    "* Bytes written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if throughput_df.empty:\n",
    "    print(\"No throughput data to display.\")\n",
    "else:\n",
    "    # Create heatmaps for each throughput metric.\n",
    "    nsys_display.display_heatmaps(\n",
    "        throughput_df,\n",
    "        throughput_metrics,\n",
    "        xaxis_title=\"Duration (s)\",\n",
    "        yaxis_title=\"Rank/Hostname/Volume/Device\",\n",
    "        zaxis_title=\"Bytes\",\n",
    "        title=\"Throughput (bins=REPLACE_BIN)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency\n",
    "\n",
    "This line graph displays the summary of all latency metrics on NFS file systems for the profiled volumes:\n",
    "* x axis represents the rank duration, scaling from 0 to the maximum duration across all ranks.\n",
    "* y axis represents the mean latency in milliseconds for read and write operations across all ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latency DataFrame from the parquet file\n",
    "latency_df = pd.read_parquet(\"latency_analysis.parquet\")\n",
    "\n",
    "if latency_df.empty:\n",
    "    display(Markdown(\"No latency data to display.\"))\n",
    "else:\n",
    "    # Create unique name.\n",
    "    latency_df[\"Name\"] = (\n",
    "        latency_df[\"Rank\"].astype(str)\n",
    "        + \"/\"\n",
    "        + latency_df[\"Hostname\"].astype(str)\n",
    "        + \"/\"\n",
    "        + latency_df[\"Volume\"].astype(str)\n",
    "        + \"/\"\n",
    "        + latency_df[\"Name\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # Convert ns to seconds\n",
    "    latency_df[\"Duration\"] = latency_df[\"Duration\"] * 1e-9\n",
    "\n",
    "    latency_metrics = [\n",
    "        \"Read RPC queue\",\n",
    "        \"Read RPC RTT\",\n",
    "        \"Read RPC exe\",\n",
    "        \"Write RPC queue\",\n",
    "        \"Write RPC RTT\",\n",
    "        \"Write RPC exe\",\n",
    "    ]\n",
    "\n",
    "    nsys_display.display_summary_graph(\n",
    "        latency_df,\n",
    "        latency_metrics,\n",
    "        xaxis_title=\"Duration (s)\",\n",
    "        yaxis_title=\"Value\",\n",
    "        title=\"Latency Summary (bins=REPLACE_BIN)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These heatmaps display latency on NFS filesystem which are collected using NVTX counters\n",
    "via the storage-metrics plugin and the --storage-metrics feature:\n",
    "* x axis represents the rank duration, scaling from 0 to the maximum duration across all ranks.\n",
    "* y axis represents the set of Rank/Hostname/Volume/Device name for which metrics were collected.\n",
    "\n",
    "The heatmaps present:\n",
    "* Read RPC queue, Read RPC RTT, Read RPC exe latencies in milliseconds\n",
    "* Write RPC queue, Write RPC RTT, Write RPC exe latencies in milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if latency_df.empty:\n",
    "    print(\"No latency data to display.\")\n",
    "else:\n",
    "    # Create heatmaps for read and write latency metrics.\n",
    "    nsys_display.display_heatmaps(\n",
    "        latency_df,\n",
    "        latency_metrics,\n",
    "        xaxis_title=\"Duration (s)\",\n",
    "        yaxis_title=\"Rank/Hostname/Volume/Device\",\n",
    "        zaxis_title=\"Latency duration (ms)\",\n",
    "        title=\"Latency (bins=REPLACE_BIN)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table associates each rank number with the original filename. Ranks are assigned assuming that\n",
    "the file names include the rank with sufficient zero padding for proper sorting. Otherwise, the\n",
    "actual rank may differ from the assigned ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = pd.read_parquet(\"files.parquet\")\n",
    "display(files_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
